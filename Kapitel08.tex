\appendix

\newchapter{Funktionalanalysis}
\label{anhang:A}

\section{Sobolev-Räume}
\label{anhang:A.1}

Sei im Weiteren $\emptyset \not= \Omega \subset \R^n$. Wir definieren den Sobolev-Raum allgemein wie folgt (vgl. \cite{BraeFEM} Kaptitel II, \S 2 und \cite{Walker} Kapitel 6).

\begin{defi}\label{def:A.1}
  Seien $1\leq p\leq\infty$ und $m\in\N$. Die Menge
  \[
  W_p^m(\Omega):=\left(
    \{u\in L_p(\Omega)\with\partial^\alpha u\in L_p(\Omega)\,\fa\,\abs a\leq m\}
    , \norm \cdot_{W_p^m}
  \right)
  \]
  heißt \textit{\idx{Sobolev-Raum}} der Ordnung $m$. Dabei ist 
  \[
  \norm u_{W_p^m}:=\norm u_{W_p^m(\Omega)}:=
  \left(
    \sum_{\abs\alpha\leq m}\norm{\partial^\alpha u}_{L_p}^p
  \right)^{\frac 1p},
  \]
  wenn $1\leq p<\infty$. Im Fall $p=\infty$ ist $\norm u_{W_p^m}:=\max_{\abs\alpha\leq m}\norm{\partial^\alpha u}_\infty$. 
  
  Weiterhin bezeichne $L_p(\Omega)$ den \textit{\idx{Lebesgue-Raum}}, d.h. den Raum der messbaren Funktionen, deren $p$-te Potenz Lebesgue-integrierbar über $\Omega$ ist, d.h.
  \[
  	L_p(\Omega) := \left( \{u : \Omega \ra \R \with f \text{ messbar}, \norm{\cdot}_{L_p} < \infty\},\norm{\cdot}_{L_p} \right) \, ,
  \]
  wobei $\norm{u}_{L_p} := \norm{u}_{L_p(\Omega)} =\norm{u}_{W^0_p}$.
\end{defi}

\begin{defi}\label{def:A.2}
Der Raum
\[
	\mcal D(\Omega) := C_c^\infty (\Omega)= \{ \varphi \in C^\infty(\Omega) \with \supp (\varphi) \subset\subset \Omega \}
\]
heißt der \textit{\idx{Raum der Testfunktionen}}, wobei $K\subset\subset \Omega :\Lra  \bar K \subset \Omega$ kompakt.
\end{defi}

\begin{bem}\label{bem:A.3}
 Seien $u\in W_p^m(\Omega)$, $\varphi\in\D(\Omega)$ und $\alpha\in\N^n$ mit $\abs\alpha\leq m$. Dann  bezeichnen wir $v = \partial^\alpha u$ als \textit{\idx{schwache Ableitung}} von $u$, wenn gilt
    \[
      \int_\Omega v\cdot\varphi \, d x=(-1)^{\abs\alpha}\int_\Omega u\cdot\partial^\alpha\varphi \,  d x\, .
    \]
\end{bem}

\begin{bsp}\label{bsp:A.4}
Es sei $\Omega = (-1,1) \subset \R$ und $u (x) = \abs x \in L_2(\Omega)$. Betrachten wir $v (x) = \sign (x)$, so ergibt sich für $\varphi \in \mcal D(\Omega)$
\begin{align*}
	\int_\Omega v \cdot \varphi \, dx & = \int_{-1}^0 -1 \cdot \varphi(x) \, dx + \int_0^1 1\cdot \varphi(x) \, dx  \\
	&= -x\varphi(x)\Big|_{-1}^0 - \int_{-1}^0 -x \varphi'(x) \, dx +  x \varphi(x)\Big|_0^1 -\int_0^1 x \varphi'(x) \, dx \\
	& = - \int_{-1}^1 \abs x \varphi'(x) \, dx = (-1)^1 \int_\Omega u \cdot \varphi' \, dx \, ,
\end{align*}
da $\varphi(-1) = \varphi(1) = 0$. Also ist $v = \partial u$ und somit $u \in W^1_2(\Omega)$. Analog kann man nachrechnen, dass 
\[
	\int_\Omega v \cdot \varphi' \, dx = -2\varphi(0)
\]
ist und somit $u$ nicht zweimal schwach ableitbar ist, d.h. $u \not \in W^2_2(\Omega)$.
\end{bsp}

Wir wollen in der Theorie der Finiten Elemente Methode vor allem Sobolev-Räume über dem Raum $L_2(\Omega)$ betrachten, daher ist folgender Satz essentiell.

\begin{satz}\label{satz:A.5}
Es seien $1\le p \le \infty$ und $m \in \N$. Dann gilt:
\begin{enumerate}[\rm (a)]
\item $W_p^m(\Omega)$ ist ein Banachraum.
\item $H^m(\Omega) := W_2^m(\Omega)$ ist ein Hilbertraum mit Skalarprodukt
\[
	(u,v)_m := (u,v)_{H^m(\Omega)} := \sum_{\abs \alpha \le m} (\partial^\alpha u,\partial^\alpha v)_0\quad \forall \, u,v \in H^m(\Omega) \, ,
\]
wobei
\[
	(u,v)_0 := (u,v)_{L_2(\Omega)} := \int_\Omega uv \, dx \, .
\]
\end{enumerate}
\end{satz}

\begin{bem}\label{bem:A.6}
\begin{enumerate}[(a)]
\item Die Norm auf $H^m(\Omega)$ ergibt sich analog zur Norm des allgemeinen Sobolev-Raumes durch das Skalarprodukt, d.h. $\norm{u}_m: = \norm u_{H^m(\Omega)} := \norm u_{W^m_2}$.
\item Analog dazu definieren wir die Halbnorm $\abs\cdot_m$ auf $H^m$ wie folgt:
\[
		\abs{u}_m: = \abs u_{H^m(\Omega)} :=  \left(
    \sum_{\abs\alpha= m}\norm{\partial^\alpha u}^2_{L_2}
  \right)^{\frac 12}.
\]
\end{enumerate}
\end{bem}

\begin{defi}\label{def:A.7}
Der Raum $H^m_0(\Omega)$ ist die Vervollständigung von $\mcal D(\Omega)$ bzgl. der Norm $\norm\cdot_m$.
\end{defi}

\begin{bem}\label{bem:A.8}
Die Funktionen $u \in H^m_0(\Omega)$ können als die Funktionen $u \in H^m(\Omega)$ mit $u = 0$ auf $\partial \Omega$ aufgefasst werden. Weiter ist $H^m_0(\Omega)$ ein abgeschlossener Unterraum von $H^m(\Omega)$ (vgl. auch \cite{Walker} Bemerkung 6.7).
\end{bem}



\section{Optimalitätskriterien}
\label{anhang:A.2}

Zunächst definieren wir einen verallgemeinerten Begriff der Richtungsableitung, der auch auf unendlich dimensionalen Vektorräumen existiert.

\begin{defi}\label{def:Gateaux-Ableitung}
Es seien $V$ ein Vektorraum, $M\subset V$ und $W$ ein normierter Raum, sowie $F: M \ra W$ eine Abbildung, $x_0 \in M$ und $v \in V$. Dann heißt $F$ \textit{\idx{Gâteaux-differenzierbar}} (bzw. in Richtung $v$ an der Stelle $x_0$ differenzierbar), falls es ein $\eps >0$ mit $[x_0-\eps v, x_0 + \eps v] \subset M$ gibt und der Grenzwert
\begin{align}\label{eq:A.1}
	\mscr D_v F (x_0) :=\frac d{d t} F(x_0+tv)\Big|_{t=0} := \lim_{t\ra 0} \frac{F(x_0+tv)-F(x_0)}t
\end{align}
in $W$ existiert. $\mscr D_vF(x_0)$ heißt dann \textit{\idx{Gâteaux-Ableitung}} von $F$ an der Stelle $x_0$ in Richtung $v$.

Falls wir nur $[x_0,x_0 + \eps v] \subset M$ voraussetzen, so können wir in \eqref{eq:A.1} $\lim_{t\ra 0}$ durch $\lim_{t \ra + 0}$ ersetzen. Dann nennen wir \eqref{eq:A.1} die \textit{rechtsseitige Gâteaux-Ableitung}\index{Gâteaux-Ableitung!rechtsseitig} und bezeichnen diese mit $\mscr D^+_v F(x_0)$.
\end{defi}

Für die Variationsrechnung sind folgende zwei Sätze für uns von besonderer Bedeutung.

\begin{satz}\label{satz:A.10}
\textnormal{(Charakterisierungssatz der konvexen Optimierung)} Es seien $M \subset V$ eine konvexe Menge, $V$ ein Vektorraum und $F : M \ra \R$ ein konvexes Funktional. Dann gilt für $x_0, x \in M$:

$x_0$ ist Lösung von $\min\limits_{x \in M} F(x)$ genau dann, wenn für alle $x \in M$ gilt
\[
	\mscr D^+_{x-x_0} F(x_0) \ge 0 \, .
\]
\end{satz}

\begin{proof}
Siehe \cite{GopRieTam}, Kapitel 3.3.3, Satz 3.34.
\end{proof}

\begin{satz}
Es sei $U\subset V$ ein (Unter-)Vektorraum, $V$ ein Vektorraum und $F: U \ra \R$ eine \idx{Gâteaux-differenzierbar}e konvexe Funktion. Dann ist $x_0 \in U$ genau dann Lösung von $\min\limits_{x \in U} F(x)$, wenn für alle $u \in U$ gilt
\[
	\mscr D_u F(x_0) = 0 \, .
\]
\end{satz}

\begin{proof}
Siehe \cite{GopRieTam}, Kapitel 3.3.3, Satz 3.35.
\end{proof}


\section{Konvergenzbegriffe}
\label{anhang:A.3}

\begin{defi}\label{defi:A.12}
Es sei $m\in \N, 1\leq p < \infty, 1 = \frac 1 p + \frac 1 {p'}$.
\begin{enumerate}[(a)]
\item Eine Folge $(u_j)$ in $L_p$ konvergiert schwach gegen $u \in L_p(\Omega)$
\begin{align*}
	&: \Longleftrightarrow u_j \rightharpoonup u \text{ in } L_p(\Omega) \\
	& : \Longleftrightarrow \, \fa \, v \in L_{p'}(\Omega) : \int_\Omega u_j v \d x \longrightarrow \int_\Omega uv \d x \text{ in } \K \, .
\end{align*}
\item Eine Folge $(u_j) \in W^m_p(\Omega)$ konvergiert schwach gegen $u \in W_p^m (\Omega)$
\begin{align*}
	& : \Longleftrightarrow u_j \rightharpoonup u \text{ in } W_p^m(\Omega) \\
	& : \Longleftrightarrow \partial^\alpha u_j \rightharpoonup \partial^\alpha u \text{ in } L_p(\Omega) \, \forall \, \abs\alpha \leq m \, .
\end{align*}
\end{enumerate}
\end{defi}


\begin{bem}
\label{bem:A.13}
Sei $1\leq p < \infty, m \in \N$, dann ist:
\begin{enumerate}[(a)]
\item Ist $u_j \ra u$ in $W^m_p(\Omega)$, dann folgt $u_j \rightharpoonup u$ in $W^m_p(\Omega)$, d.h. "`starke Konvergenz ist stärker als schwache Konvergenz"'.
\begin{proof}
$\fa \, v \in L_{p'}(\Omega), \abs \alpha \leq m$ gilt
\[
	\Abs{\int_\Omega (\partial^\alpha u_j - \partial^\alpha u) v \d x} \stackrel[\scriptsize\text{Hölder}]{}\leq \norm v_{L_{p'}(\Omega)} \norm{\partial^\alpha u_j -\partial^\alpha u }_{L_p(\Omega)} \longrightarrow 0 \, .\qedhere
\]
\end{proof}
\item Sei $1 < p < \infty, (u_j)\subset W^m_p(\Omega)$ beschränkt (bzgl. $\norm{\, \cdot\, }_{W^m_p}$), dann folgt, dass eine Teilfolge $(u_{j'})$ und ein $u \in W^m_p(\Omega)$ existiert, so dass $u_{j'} \rightharpoonup u$ in $W^m_p(\Omega)$, d.h. "`beschränkte Folgen sind relativ schwach kompakt"'.
\begin{proof}
Vgl. \cite{Rudin}.
\end{proof}
\item Es sei $M\subset W^m_p(\Omega)$ konvex und abgeschlossen (bzgl. $\norm{\, \cdot\, }_{W^m_p}$), sowie $(u_j) \subset M$ mit $u_j \rightharpoonup u $ in $W^m_p(\Omega)$, dann ist $u \in M$, d.h. "`abgeschlossene konvexe Mengen sind schwach abgeschlossen"' (Theorem von Mazun; ohne Beweis, vgl. \cite{Rudin}).
\item Es sei $u_j \rightharpoonup u $ in $W^p_m (\Omega)$, dann folgt $(u_j)$ ist beschränkt in $W^m_p(\Omega)$ (bzgl. $\norm{\, \cdot\, }_{W^m_p}$), d.h. "`schwach konvergente Folgen sind beschränkt"'.
\begin{proof}
Theorem von Mackey, vgl. \cite{Rudin}.
\end{proof}
\item $u_ j \rightharpoonup u $ in $W^m_p(\Omega), u_j \rightharpoonup v$ in $W^m_p(\Omega)$, dann gilt $u=v$, d.h. "`Grenzwerte von schwach konvergenten Folgen sind eindeutig"'.
\begin{proof}
Aus dem Hauptsatz der Variationsrechnung folgt die Behauptung.
\end{proof}
\item Sei $u_j \rightharpoonup u $ in $W^m_p(\Omega)$, dann folgt $\norm u_{W^m_p(\Omega)} \leq \lim \inf \norm{u_j}_{W^m_p(\Omega)}$.
\end{enumerate}
\end{bem}

\begin{theorem}\label{theorem:A.14}
In einem reflexiven Raum $V$\index{reflexiver Raum}, d.h. der Bidualraum $V''$ ist isomorph zu $V$, besitzt jede beschränkte Folge $(v_n)_{n\in \N}$ eine schwach konvergente Teilfolge $(v_{n_j})$.
\end{theorem}

\begin{proof}
Der Beweis befindet sich in \cite{Werner} Kapitel III, Theorem 3.7.
\end{proof}

\begin{bem}\label{bem:A.15}
Jeder Hilbertraum $H$ ist reflexiv.
\end{bem}

\begin{proof}
Dies folgt aus dem Darstellungssatz von Riesz (Satz \ref{satz:2.14}).
\end{proof}










\newchapter{Optimierung}
\label{anhang:B}

\section{Quadratische Programmierung}
\label{anhang:B.1}

Um im folgenden die Idee des Algorithmus zu verstehen, führen wir zunächst grundlegende Begriffe ein. Ein quadratisches Problem mit Gleichungs- und Ungleichungsnebenbedingungen ist von der Form
\begin{align}\label{eq:QP}
\begin{aligned}
	\min_{\bs x} & \quad q(\bs x) = \frac 1 2 \bs x^T G\bs x + \bs x^T \bs c \\
	\text{s.t.} & \quad \bs a_i^T \bs x = b_i \, , \quad i \in \mcal E, \\
	& \quad \bs a_i^T \bs x\ge b_i \, , \quad i \in \mcal I,
\end{aligned}
\end{align}
wobei $\mcal E$ und $ \mcal I$ die Indexmengen der Gleichungs- und Ungleichungsnebenbedingungen darstellen und $\bs c,\bs x,\bs a_i \in \R^n, b_i \in \R, i \in \mcal E \cup \mcal I$, sowie $G$ eine symmetrische $(n\times n)$-Matrix ist, welche die Hesse-Matrix des Problems darstellt. Damit ist die Hesse-Matrix konstant und daher das Problem konvex, wenn $G$ positiv semidefinit ist. (Ist $G$ positiv definit, so nennen wir das Problem strikt konvex. Wenn $G$ indefinit ist, ist \eqref{eq:QP} "`nicht konvex"'.)

Da sonst das quadratische Problem (und damit der Active-Set Algorithmus) zu kompliziert wird, betrachten wir hier nur den konvexen Fall. Für diesen Fall können wir leicht zeigen, dass eine Lösung $\bs x^*$, die die Bedingungen 1. Ordnung erfüllt, auch globale Lösung des Problems ist (s. Theorem \ref{A.1}). Anschaulich kann es im indefiniten Fall mehrere optimale Punkte geben, die voneinander getrennt liegen, d.h. die Menge der optimalen Punkte ist nicht zusammenhängend, wodurch das Auffinden des globalen Minimums erschwert wird.

Die notwendigen Bedingungen 1. Ordnung sind die KKT-Bedingungen und können hier angewendet werden, da die Restriktionen und die Zielfunktion stetig differenzierbar sind. Die Lagrangefunktion $\mcal L$ für das quadratische Problem ist
\begin{align}
	\mcal L(\bs x,\bs\lambda) = \frac 1 2 \bs x^T G \bs x + \bs x^T \bs c- \sum_{i \in \mcal I \cup \mcal E} \lambda_i (\bs a_i^T \bs x-b_i) \, .
\end{align}
Damit ergeben sich –  vgl. \cite{NocWri}, Theorem 12.1 – mit der Menge der aktiven Nebenbedingungen $\mcal A(\bs x^*) = \{i\in \mcal E \cup \mcal I : \bs a_i^T \bs x^* = b_i\}$ die KKT-Bedingungen
\begin{align}\label{eq:KKT}
\begin{aligned}
	\nabla_{\bs x} \mcal L(\bs x^*,\bs \lambda^*) & = G\bs x^*+\bs c-\sum_{i \in \mcal A(\bs x^*)} \lambda^*_i \bs a_i  = 0 \, , \\
	\bs a_i^T \bs x^* &  = b_i \, , \quad \forall \, i \in \mcal A(\bs x^*) , \\
	\bs a_i^T \bs x^* &  \ge b_i \, , \quad \forall \, i \in \mcal I \setminus \mcal A(\bs x^*) ,\\
	\lambda_i^* & \ge 0 \, , \quad \, \forall i \in \mcal I \cap \mcal A(\bs x^*) .
\end{aligned}
\end{align}
Hierbei ist $\bs x^*$ Lösung von \eqref{eq:QP} und erfüllt die LICQ-Bedingung; $\bs\lambda^*$ ist dazugehöriger optimaler Lagrange-Multiplikator. In \eqref{eq:KKT} wird die Komplementaritätsbedingung $\lambda^*_i c_i(\bs x^*) = 0$ impliziert durch $\lambda_i^* = 0 \, \forall \, i \not\in \mcal A(\bs x^*)$.

\begin{theorem}\label{theorem:B.1}
Wenn $\bs x^*$ die Bedingungen \textnormal{\eqref{eq:KKT}} erfüllt mit $\lambda_i^*,i \in \mcal A(\bs x^*)$ und $G$ ist positiv semidefinit, dann ist $\bs x^*$ eine globale Lösung von \textnormal{\eqref{eq:QP}}.
\end{theorem}

\begin{proof}
Wenn $\bs x$ ein beliebiger weiterer zulässiger Punkt für (1.1) ist, gelten die Restriktionen $\bs a_i^T\bs x = b_i,  i \in \mcal E$,  sowie $\bs a_i^T \bs x \ge b_i, i \in \mcal I \cap \mcal A(\bs x^*)$ für $\bs x$ und damit gilt zusammen mit der ersten Bedingung von \eqref{eq:KKT}, dass
\[
	(\bs x-\bs x^*)^T (G\bs x^*+\bs c) = \sum_{i \in \mcal E} \underbrace{\lambda^*_i \bs a_i^T (\bs x-\bs x^*)}_{\ge 0} + \sum_{i \in \mcal A(x^*)\cap \mcal I} \underbrace{\lambda^*_i \bs a_i^T (\bs x-\bs x^*)}_{\ge 0} \ge 0 \, .
\]
Dann drücken wir $q(\bs x)$ durch $q(\bs x^*)$ aus und wenden die obere Ungleichung sowie die positive Semidefinitheit für $G$ an, um zu zeigen, dass $q(\bs x) \ge q(\bs x^*)$ ist. Damit ist $\bs x^*$ globale Lösung des quadratischen Problems.
\end{proof}

Daher ist im positiv semidefiniten Fall gesichert, dass ein optimaler Punkt auch gleichzeitig globale Lösung ist.


\section{Active Set-Methode für konvexe QPs}
\label{anhang:B.2}

Wenn wir eine Lösung $\bs x^*$ für das Problem \eqref{eq:QP} kennen, so ist auch die Menge der aktiven Nebenbedingungen $\mcal A(x^*)$ bekannt und wir können \eqref{eq:QP} vereinfachen zum Optimierungsproblem
\begin{align}\label{eq:active}
	\min_{\bs x} & \quad q(\bs x) = \frac 1 2 \bs x^T G \bs x+\bs x^T \bs c \, , \quad	\text{s.t.} \quad \bs a_i^T\bs x = b_i \, , \quad i \in \mcal A(\bs x^*)\, .
\end{align}
Dieses könnten wir dann beispielsweise mit direkten Verfahren wie der Schur-Komplement-Methode oder der Nullraum-Methode lösen. Natürlich ist die optimale Lösung zu Beginn noch nicht bekannt und damit auch nicht die aktiven Restriktionen. Jedoch können wir diese Idee für die Active-Set-Methode verwenden.

Das Hauptziel der Active-Set-Methode ist, die Menge der aktiven Restriktionen bzgl. der optimalen Lösung zu finden, wobei wir hier die primale Variante betrachten wollen, in der die Approximierte $\bs x_k$ zulässig bzgl. des primalen Problems ist. 

Die Grundidee ist, ein quadratisches Teilproblem zu lösen, bei dem wir bestimmte Nebenbedingungen aus Problem \eqref{eq:QP} bzgl. $\mcal I$ als aktiv annehmen. Die dadurch beschriebene Indexmenge der aktiven Restriktionen für $\bs x_k$ im $k$-ten Schritt heißt \textit{working set} und kann wie folgt beschrieben werden
\[
	\mcal W_k = \{ i \, | \, \bs a_i^T \bs x_k = b_i,  i \in \mcal E \cup \mcal J, \mcal J \subset \mcal I\} \, .
\]
Hierbei muss vorausgesetzt werden, dass die Nebenbedingungen in $\mcal W_k$ die LICQ-Bedingung erfüllen, selbst wenn diese bezogen auf alle Nebenbedingungen an der Stelle $x_k$ nicht erfüllt wird.

Wir betrachten nun den $k$-ten Schritt mit der Approximierten $\bs x_k$ und dem working set $\mcal W_k$. Wir berechnen die neue Iterierte $\bs x_{k+1}$, indem wir eine Richtung $\bs p$ finden, in der wir unter den Nebenbedingungen $\mcal W_k$ die Funktion $q$ minimieren. Hierfür betrachten wir $\bs x_{k+1} = \bs x_k + \bs p$ und setzen $\bs x_{k+1}$ in $q$ ein:
\begin{align*}
	 q(\bs x_{k+1}) & = q(\bs x_k+\bs p) = \frac 1 2 (\bs x_k + \bs p)^T G (\bs x_k + \bs p) + (\bs x_k + \bs p)^T \bs c \\
	& = \frac 1 2 \bs x^T_k G \bs x_k + \underbrace{\bs x_k^T G \bs p}_{\text{da $G$ symm.}} + \frac 1 2 \bs p^T G\bs p +\bs x_k^T \bs c +\bs p^T \bs c \\
	& = \frac 1 2\bs p^T G \bs p +\bs  g_k^T \bs p + \rho_k \, ,
\end{align*}
wobei $\bs g_k = G\bs x_k+\bs c$ und $\rho_k = \frac 1 2 \bs x_k^T G \bs x_k + \bs x_k^T \bs c$. Da wir den Parameter $\bs p$ so wählen wollen, so dass $q(\bs x_{k+1})$ minimal wird, ist der Term $\rho_k$ bzgl. des Problems konstant und  kann somit für die Lösung jenes weggelassen werden. Da weiterhin auch $\bs x_{k+1}$ die aktiven Nebenbedingungen $\mcal W_k$ erfüllen soll, gilt
\[
	\bs a_i^T \bs p =\bs a_i^T (\bs x_{k+1} - \bs x_k) = \underbrace{\bs a_i^T\bs x_{k+1}}_{=b_i} - \underbrace{\bs a_i^T \bs x_k}_{=b_i} = 0 \quad \forall \, i \in \mcal W_k \, .
\]
Zusammengefasst müssen wir also im $k$-ten Schritt das Teilproblem
\begin{align}\label{eq:subprob}
\begin{aligned}
	\min_{\bs p} & \quad \frac 1 2\bs p^T G \bs p + \bs g_k^T \bs p \, , \\
	\text{s.t.} & \quad \bs a_i^T\bs p = 0 \, , \quad \forall i \in \mcal W_k 
\end{aligned}
\end{align}
lösen. Die Lösung im $k$-ten Schritt von \eqref{eq:subprob} bezeichnen wir mit $\bs p_k$. Umgekehrt gilt damit, analog zur obigen Rechnung, natürlich auch, dass für alle $i \in \mcal W_k$ die Restriktion aktiv bleibt für $\bs x_k + \alpha \bs p_k$ mit beliebigem $\alpha$. Da $G$ positiv definit ist, kann \eqref{eq:subprob} nun – wie schon bei \eqref{eq:active} erwähnt – mit Schur-Komplement-Methode oder Nullraum-Methode gelöst werden.

Wie wir schon wissen, ist die neue Iterierte $\bs x_{k+1} = \bs x_k + \bs p_k$ bzgl. $\mcal W_k$ immer noch zulässig. Nun müssen wir jedoch feststellen, ob diese Iterierte auch alle übrigen Restriktionen mit $i\not\in \mcal W_k$ erfüllt. Ist dies der Fall, so setzen wir $\bs x_{k+1} = \bs x_k +\bs p_k$, ansonsten suchen wir das größtmögliche $\alpha_k \in [0,1]$, so dass
\[
	\bs x_{k+1} =\bs x_k + \alpha_k \bs p_k
\] 
zulässig bleibt. Hierfür betrachten wir zwei Fälle.

\underline{Fall 1:} Gilt für ein $i \not \in \mcal W_k$, dass $\bs a_i^T \bs p_k \ge 0$ ist, so folgt
\[
	\bs a_i^T (\bs x_k + \alpha_k \bs p_k) =\bs a_i^T \bs x_k + \underbrace{\alpha_k \bs a_i^T\bs p_k}_{\ge 0} \ge \bs a_i^T \bs x_k \ge b_i \, ,
\]
da $\alpha_k \ge 0$, d.h. für diese Nebenbedingungen müssen wir für die Wahl von $\alpha_k$ nichts beachten.

\underline{Fall 2:} Existiert ein $i \not \in \mcal W_k$, für das $\bs a_i^T\bs p_k < 0$ ist, so gilt
\begin{align}\label{eq:alpha}
\notag	& \bs a_i^T (\bs x_k + \alpha_k \bs p_k) \ge b_i \\
\notag	\Llra \quad &\bs a_i^T \bs x_k + \alpha_k \bs a_i^T\bs p_k \ge b_i \\
\notag	\Llra \quad & \alpha_k \underbrace{\bs a_i^T\bs p_k}_{< 0} \ge b_i - \bs a_i^T \bs x_k \\
	\Llra \quad &  \alpha_k \le \frac{b_i - \bs a_i^T\bs x_k }{\bs a_i^T \bs p_k} \, .
\end{align}

Damit folgt mit \eqref{eq:alpha} und den vorherigen Überlegungen, dass zusammengefasst
\begin{align}\label{eq:alpha_k}
	\alpha_k = \min\left\{ 1,\min_{i \not\in \mcal W_k, \bs a_i^T\bs p_k < 0}  \frac{b_i -\bs a_i^T\bs x_k }{\bs a_i^T \bs p_k}  \right\}
\end{align}
gilt. Eine Restriktion $i \not \in \mcal W_k$, für die das Minimum für $\alpha_k$ angenommen wird, nennen wir \textit{blocking constraint}; diese muss nicht eindeutig sein, da wir beispielsweise anschaulich auch von einer Ecke geblockt werden können. Ist $\alpha_k = 1$, so werden alle Restriktion außerhalb vom {working set} mit dem Schritt $\bs x_{k+1} =\bs x_k + \bs p_k$ erfüllt, d.h. es gibt keine {blocking constraint}. Gibt es eine Nebenbedingung $j \not\in \mcal W_k$, die aktiv ist, obwohl sie nicht zum working set gehört, so gilt
\begin{align*}
	\alpha_k & = \min\left\{ 1,\min_{i \not\in \mcal W_k, \bs a_i^T\bs p_k < 0}  \frac{b_i -\bs a_i^T\bs x_k }{\bs a_i^T\bs p_k}  \right\}  \\
	& = \min\left\{ 1, \frac{b_j - {\bs a_j^T \bs x_k}}{\bs a_j^T\bs p_k}  \right\}  \\
	&  = \min \left\{1,\frac{b_j-b_j}{\bs a_j^T\bs p_k}\right\} = 0 \, .
\end{align*}
Es sei $j \not \in \mcal W_k$ nun ein Index einer {blocking constraint}. Dann ist
\[
	\bs x_{k+1} =\bs x_k + \alpha_k \bs p_k = \bs x_k + \frac{b_j - {\bs a_j^T \bs x_k}}{\bs a_j^T\bs p_k}\bs p_k \, .
\]
Setzen wir $\bs x_{k+1}$ in die $j$-te Restriktion ein, so erhalten wir
\begin{align*}
	\bs a_j^T\bs x_{k+1} & = \bs a_j^T \( \bs x_k +  \frac{b_j - \bs a_j^T\bs x_k}{\bs a_j^T\bs p_k}\bs p_k  \) = \bs a_j^T\bs x_k + \frac{b_j - {\bs a_j^T\bs x_k}}{\cancel{\bs a_j^T\bs p_k}} \cdot \cancel{\bs a_j^T\bs p_k} \\
	& =\bs a_j^T\bs x_k + b_j -\bs a_j^T\bs x_k = b_j \, ,
\end{align*}
d.h. die {blocking constraint} ist für die neue Iterierte $\bs x_{k+1}$ nach Konstruktion aktiv. Daher setzen wir als neues {working set} $\mcal W_{k+1} = \mcal W_k \cup \{ j\}$.

Das oben beschriebene Vorgehen wiederholen wir so lange, bis wir das {working set} $\hat {\mcal W}$ mit dem Minimum des quadratischen Problems $\hat{\bs x}$ gefunden haben. Dies ist leicht zu erkennen, da wir \eqref{eq:QP} auf $\mcal W_k$ nicht weiter minimieren können, sobald es keinen Schritt $p$ gibt, in dessen Richtung wir $q$ verringern können, d.h. wenn $\bs p = \bs 0$ die Lösung für das Teilproblem \eqref{eq:subprob} ist.  Dann ist der optimale Punkt $\hat{\bs x}$ bzgl. des {working sets} $\hat{\mcal W}\subset \mcal A(\hat{\bs x})$ gefunden.

Wir müssen jetzt überprüfen, ob $\hat{\bs x}$ die KKT-Bedingungen erfüllt. Wir wissen, dass für $\bs p= \bs 0$ die KKT-Bedingungen für \eqref{eq:subprob}
\begin{align*}
\begin{pmatrix}
	G & A^T \\
	A & 0
\end{pmatrix} \cdot
\begin{pmatrix} 
-\bs p \\
\hat{\bs\lambda}
 \end{pmatrix} =
 \begin{pmatrix}
 	\hat{\bs g} \\
	\hat{\bs h}
 \end{pmatrix}
\end{align*}
mit $\hat{\bs g} = \bs c + G\hat{\bs x}, \bs h = A\hat{\bs x}+\bs b$ und $\bs p=\bs 0$ erfüllt. Daraus folgt
\begin{align*}
	A^T \hat{\bs \lambda} = \hat{\bs g}  \quad &\Llra \quad \sum_{i \in \hat{\mcal W}}\bs a_i \hat\lambda_i = G\hat{\bs x} +\bs c \, , \\
	\bs 0 = \hat{\bs h} \quad  & \Llra \quad A\hat{\bs x} =\bs b\, , 
\end{align*}
wobei $A$ die Gradienten $\bs a_i^T$ der aktiven Restriktionen $\hat{\mcal W}$ zeilenweise  enthält. Damit werden die ersten beiden KKT-Bedingungen aus \eqref{eq:KKT} erfüllt. Da die Schrittlänge $\alpha_k$ mit \eqref{eq:alpha} so gewählt ist, dass die übrigen Restriktionen erfüllt bleiben, gilt auch die dritte Bedingung aus \eqref{eq:KKT}. Es bleibt zu überprüfen, ob die Lagrange-Multiplikatoren $\hat\lambda_i \ge 0$ sind.

Gilt $\hat\lambda_i \ge 0$ für alle $i \in \hat{\mcal W} \cap \mcal I$, so sind alle KKT-Bedingungen erfüllt und damit $\bs x^* = \hat{\bs x}$. Existiert allerdings ein $j \in \hat{\mcal W} \cap \mcal I$, so dass $\hat \lambda_j < 0$ ist, so können wir den Wert von $q$ noch weiter verringern, indem wir die $j$-te Restriktion wegfallen lassen (vlg. \cite{NocWri}, Kapitel 12.3). Dies zeigt das folgende Theorem.

\begin{theorem}
Der Punkt $\hat{\bs x}$ erfülle die notwendigen Bedingungen 1. Ordnung für das Teilproblem \textnormal{\eqref{eq:subprob}} auf $\hat{\mcal W}$. Weiter seien die Gradienten $\bs a_i, i \in \hat{\mcal W}$, linear unabhängig $($LICQ$)$ und es gebe einen Index $j \in \mcal W$ mit $\hat \lambda_j<0$. Es sei $\bs p$ die Lösung vom Teilproblem \textnormal{\eqref{eq:subprob}} ohne die Restriktion $j$, d.h.
\begin{align*}
	\min_{\bs p} & \quad \frac 1 2\bs p^T G\bs p + (G\hat{ \bs x} +\bs c)^T\bs p \, , \\
	\textnormal{s.t.} & \quad\bs a_i^T\bs p = 0 \, , \quad  \forall \, i \in \hat{\mcal W} \setminus \{j\} \, .
\end{align*}
Dann ist $p$ eine zulässige Richtung für die Nebenbedingung $j$, d.h. $\bs a_j^T \bs p \ge 0$. Weiterhin gilt sogar $\bs a_j^T\bs p > 0$ und $p$ ist eine Abstiegsrichtung für $q$, wenn $\bs p$ die hinreichenden Bedingungen 2. Ordnung erfüllt.
\end{theorem}

Da wir zeigen können, dass der erzielte Abstieg für $q$ durch das Weglassen einer Nebenbedingung mit negativem Lagrange-Multiplikator $\lambda_i$ proportional zu $\abs{\lambda_i}$ ist, eliminieren wir gerade die Restriktion mit kleinstem Langrange-Multiplikator. Es kann allerdings sein, dass der folgende zu berechnende Schritt $\bs p$ aufgrund einer {blocking constraint} kurz ist, wodurch nicht garantiert ist, dass $q$ den größtmöglichen Abstieg erfährt.

%Wie oben beschrieben fordern wir, dass die Gradienten im {working set} $\mcal W_k$ linear unabhängig sind (LICQ-Bedingung). Betrachten wir nun unsere Strategie in der Active-Set-Methode, in das {working set} Restriktionen hinzuzufügen oder welche aus diesem zu eliminieren, sieht man leicht, dass die Elimination von Nebenbedingungen die lineare Abhängigkeit der Gradienten im working set nicht hervorrufen kann. Man kann zudem zeigen, dass eine {blocking constraint} immer linear unabhängig ist zu den schon bestehenden aktiven Nebenbedingungen. Damit kann auch das Hinzufügen die lineare Unabhängigkeit nicht beeinflussen.
%
%Das Eliminieren und Hinzufügen von Nebenbedingungen führt dazu, dass der in Kapitel 3 aufgerührte Algorithmus eine natürliche untere Schranke erhält. Wenn wir beispielsweise eine Lösung $x^*$ eines Problems haben, in der $m$ der Ungleichungs-Restriktionen aktiv sind, in unserer Startnäherung $x_0$ allerdings keine Ungleichungs-Restriktion, so benötigen wir mindestens $m$ Schritte, um von $x_0$ zu $x^*$ zu gelangen, da wir in jedem Schritt genau eine Nebenbedingung hinzufügen oder eliminieren.


\section{Algorithmus}
\label{anhang:B.3}

\begin{algorithm}[H]
\caption{Active-Set-Methode für konvexe quadratische Probleme}
Gegeben sei ein zulässiger Startpunkt $\bs x_0$ für \eqref{eq:QP} und definiere $\mcal W_0$ z.B. mit allen aktiven Restriktionen bzgl. $\bs x_0$.
\begin{algorithmic}
\For{k = 0,1,2,\ldots}
\State Löse \eqref{eq:subprob} zur Berechnung von $\bs p_k$;
\If {$\bs p_k = \bs 0$}
\State Berechne die Lagrange-Multiplikatoren mittels (2.5a)
\State \quad und setze $\hat{\mcal W} = \mcal W_k$;
\If{$\hat\lambda_i \ge 0 \, \forall \, i \in \hat{\mcal W} \cap \mcal I$}
\State \textbf{stop} mit der Lösung $\bs x^* = \hat{\bs x}$;
\Else
\State $j \la \arg\min_{j\in \mcal W_k \cap \mcal I} \hat\lambda_j;$
\State $\bs x_{k+1} \la \bs x_k, \mcal W_{k+1} \la W_k \setminus \{j\}$;
\EndIf
\Else \quad ($\bs p_k \not= \bs 0$)
\State Berechne $\alpha_k$ mit \eqref{eq:alpha_k};
\State $\bs x_{k+1} \la  \bs x_k + \alpha_k\bs p_k$;
\If {$\alpha_k < 1$ ({blocking constraint} existiert)}
\State Bestimme  blocking constraint $j$ und setze $\mcal W_{k+1} \la \mcal W_k \cup \{j\}$
\Else
\State $\mcal W_{k+1} \la \mcal W_k$
\EndIf
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}







\newchapter{Tensorrechnung}
\label{anhang:C}

hier auch ein paar Integralsätzer???






\newchapter{Quellcode}
\label{anhang:D}

\section{Implementierung des Fehlerschätzers für das Hindernisproblem}

%\newpage
%\newchapter{noch mehr Quellcode}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "Skript"
%%% End: 
